{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf4d274",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40b5f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_qna = pd.read_csv(\"raw_data/cooking_knowledge.csv\")\n",
    "df_recipes = pd.read_csv(\"raw_data/recipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ff6c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in qna dataset:  5647\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "54b24dfc-8ba2-44d7-8d87-ce390e8492d8",
       "rows": [
        [
         "0",
         "cooking and conservation",
         "What is HACCP?",
         "HACCP (Hazard Analysis Critical Control Point) is a preventive system ensuring food safety by identifying, assessing, and controlling hazards at every stage of food production."
        ],
        [
         "1",
         "cooking and conservation",
         "What is frying?",
         "Frying is a quick cooking method using hot oil or fat that imparts flavor and color to foods, commonly done as shallow or deep frying."
        ],
        [
         "2",
         "cooking and conservation",
         "What is Navarin?",
         "Navarin is a dark lamb stew prepared by browning the meat and then simmering it with vegetables and stock."
        ],
        [
         "3",
         "cooking and conservation",
         "What is betalin?",
         "Betalin is a pigment that imparts deep purple-red coloration to certain vegetables, such as beets."
        ],
        [
         "4",
         "cooking and conservation",
         "What is stewing?",
         "Stewing slowly cooks small pieces of food in a minimal amount of liquid, allowing flavors to blend. The tender morsels and the resulting sauce are served together."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cooking and conservation</td>\n",
       "      <td>What is HACCP?</td>\n",
       "      <td>HACCP (Hazard Analysis Critical Control Point)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cooking and conservation</td>\n",
       "      <td>What is frying?</td>\n",
       "      <td>Frying is a quick cooking method using hot oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cooking and conservation</td>\n",
       "      <td>What is Navarin?</td>\n",
       "      <td>Navarin is a dark lamb stew prepared by browni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cooking and conservation</td>\n",
       "      <td>What is betalin?</td>\n",
       "      <td>Betalin is a pigment that imparts deep purple-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cooking and conservation</td>\n",
       "      <td>What is stewing?</td>\n",
       "      <td>Stewing slowly cooks small pieces of food in a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       type          question  \\\n",
       "0  cooking and conservation    What is HACCP?   \n",
       "1  cooking and conservation   What is frying?   \n",
       "2  cooking and conservation  What is Navarin?   \n",
       "3  cooking and conservation  What is betalin?   \n",
       "4  cooking and conservation  What is stewing?   \n",
       "\n",
       "                                            response  \n",
       "0  HACCP (Hazard Analysis Critical Control Point)...  \n",
       "1  Frying is a quick cooking method using hot oil...  \n",
       "2  Navarin is a dark lamb stew prepared by browni...  \n",
       "3  Betalin is a pigment that imparts deep purple-...  \n",
       "4  Stewing slowly cooks small pieces of food in a...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in qna dataset: \",len(df_qna))\n",
    "df_qna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8769d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the recipes dataset:  2231142\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ingredients",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "directions",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "link",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "NER",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "fd5f0489-4ce7-4a36-8af3-d9db7c4e5ada",
       "rows": [
        [
         "0",
         "0",
         "No-Bake Nut Cookies",
         "[\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]",
         "[\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=44874",
         "Gathered",
         "[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"butter\", \"bite size shredded rice biscuits\"]"
        ],
        [
         "1",
         "1",
         "Jewell Ball'S Chicken",
         "[\"1 small jar chipped beef, cut up\", \"4 boned chicken breasts\", \"1 can cream of mushroom soup\", \"1 carton sour cream\"]",
         "[\"Place chipped beef on bottom of baking dish.\", \"Place chicken on top of beef.\", \"Mix soup and cream together; pour over chicken. Bake, uncovered, at 275\\u00b0 for 3 hours.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=699419",
         "Gathered",
         "[\"beef\", \"chicken breasts\", \"cream of mushroom soup\", \"sour cream\"]"
        ],
        [
         "2",
         "2",
         "Creamy Corn",
         "[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]",
         "[\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=10570",
         "Gathered",
         "[\"frozen corn\", \"cream cheese\", \"butter\", \"garlic powder\", \"salt\", \"pepper\"]"
        ],
        [
         "3",
         "3",
         "Chicken Funny",
         "[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans chicken gravy\", \"1 (10 1/2 oz.) can cream of mushroom soup\", \"1 (6 oz.) box Stove Top stuffing\", \"4 oz. shredded cheese\"]",
         "[\"Boil and debone chicken.\", \"Put bite size pieces in average size square casserole dish.\", \"Pour gravy and cream of mushroom soup over chicken; level.\", \"Make stuffing according to instructions on box (do not make too moist).\", \"Put stuffing on top of chicken and gravy; level.\", \"Sprinkle shredded cheese on top and bake at 350\\u00b0 for approximately 20 minutes or until golden and bubbly.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=897570",
         "Gathered",
         "[\"chicken\", \"chicken gravy\", \"cream of mushroom soup\", \"shredded cheese\"]"
        ],
        [
         "4",
         "4",
         "Reeses Cups(Candy)  ",
         "[\"1 c. peanut butter\", \"3/4 c. graham cracker crumbs\", \"1 c. melted butter\", \"1 lb. (3 1/2 c.) powdered sugar\", \"1 large pkg. chocolate chips\"]",
         "[\"Combine first four ingredients and press in 13 x 9-inch ungreased pan.\", \"Melt chocolate chips and spread over mixture. Refrigerate for about 20 minutes and cut into pieces before chocolate gets hard.\", \"Keep in refrigerator.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=659239",
         "Gathered",
         "[\"peanut butter\", \"graham cracker crumbs\", \"butter\", \"powdered sugar\", \"chocolate chips\"]"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  title  \\\n",
       "0           0    No-Bake Nut Cookies   \n",
       "1           1  Jewell Ball'S Chicken   \n",
       "2           2            Creamy Corn   \n",
       "3           3          Chicken Funny   \n",
       "4           4   Reeses Cups(Candy)     \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4  [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \\\n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered   \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered   \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered   \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered   \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered   \n",
       "\n",
       "                                                 NER  \n",
       "0  [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
       "1  [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
       "2  [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
       "3  [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
       "4  [\"peanut butter\", \"graham cracker crumbs\", \"bu...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in the recipes dataset: \",len(df_recipes))\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fff8717a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'title', 'ingredients', 'directions', 'link', 'source',\n",
       "       'NER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e5cca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ingredients",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "directions",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "link",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3a0b34af-5521-41c7-a827-28db2e58ed42",
       "rows": [
        [
         "0",
         "No-Bake Nut Cookies",
         "[\"1 c. firmly packed brown sugar\", \"1/2 c. evaporated milk\", \"1/2 tsp. vanilla\", \"1/2 c. broken nuts (pecans)\", \"2 Tbsp. butter or margarine\", \"3 1/2 c. bite size shredded rice biscuits\"]",
         "[\"In a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\", \"Stir over medium heat until mixture bubbles all over top.\", \"Boil and stir 5 minutes more. Take off heat.\", \"Stir in vanilla and cereal; mix well.\", \"Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\", \"Let stand until firm, about 30 minutes.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=44874",
         "Gathered"
        ],
        [
         "1",
         "Jewell Ball'S Chicken",
         "[\"1 small jar chipped beef, cut up\", \"4 boned chicken breasts\", \"1 can cream of mushroom soup\", \"1 carton sour cream\"]",
         "[\"Place chipped beef on bottom of baking dish.\", \"Place chicken on top of beef.\", \"Mix soup and cream together; pour over chicken. Bake, uncovered, at 275\\u00b0 for 3 hours.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=699419",
         "Gathered"
        ],
        [
         "2",
         "Creamy Corn",
         "[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg. cream cheese, cubed\", \"1/3 c. butter, cubed\", \"1/2 tsp. garlic powder\", \"1/2 tsp. salt\", \"1/4 tsp. pepper\"]",
         "[\"In a slow cooker, combine all ingredients. Cover and cook on low for 4 hours or until heated through and cheese is melted. Stir well before serving. Yields 6 servings.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=10570",
         "Gathered"
        ],
        [
         "3",
         "Chicken Funny",
         "[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans chicken gravy\", \"1 (10 1/2 oz.) can cream of mushroom soup\", \"1 (6 oz.) box Stove Top stuffing\", \"4 oz. shredded cheese\"]",
         "[\"Boil and debone chicken.\", \"Put bite size pieces in average size square casserole dish.\", \"Pour gravy and cream of mushroom soup over chicken; level.\", \"Make stuffing according to instructions on box (do not make too moist).\", \"Put stuffing on top of chicken and gravy; level.\", \"Sprinkle shredded cheese on top and bake at 350\\u00b0 for approximately 20 minutes or until golden and bubbly.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=897570",
         "Gathered"
        ],
        [
         "4",
         "Reeses Cups(Candy)  ",
         "[\"1 c. peanut butter\", \"3/4 c. graham cracker crumbs\", \"1 c. melted butter\", \"1 lb. (3 1/2 c.) powdered sugar\", \"1 large pkg. chocolate chips\"]",
         "[\"Combine first four ingredients and press in 13 x 9-inch ungreased pan.\", \"Melt chocolate chips and spread over mixture. Refrigerate for about 20 minutes and cut into pieces before chocolate gets hard.\", \"Keep in refrigerator.\"]",
         "www.cookbooks.com/Recipe-Details.aspx?id=659239",
         "Gathered"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>directions</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No-Bake Nut Cookies</td>\n",
       "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
       "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
       "      <td>Gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jewell Ball'S Chicken</td>\n",
       "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
       "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
       "      <td>Gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy Corn</td>\n",
       "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
       "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
       "      <td>Gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken Funny</td>\n",
       "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
       "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
       "      <td>Gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reeses Cups(Candy)</td>\n",
       "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
       "      <td>[\"Combine first four ingredients and press in ...</td>\n",
       "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
       "      <td>Gathered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                        ingredients  \\\n",
       "0    No-Bake Nut Cookies  [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
       "1  Jewell Ball'S Chicken  [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
       "2            Creamy Corn  [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
       "3          Chicken Funny  [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
       "4   Reeses Cups(Candy)    [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
       "\n",
       "                                          directions  \\\n",
       "0  [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
       "1  [\"Place chipped beef on bottom of baking dish....   \n",
       "2  [\"In a slow cooker, combine all ingredients. C...   \n",
       "3  [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
       "4  [\"Combine first four ingredients and press in ...   \n",
       "\n",
       "                                              link    source  \n",
       "0   www.cookbooks.com/Recipe-Details.aspx?id=44874  Gathered  \n",
       "1  www.cookbooks.com/Recipe-Details.aspx?id=699419  Gathered  \n",
       "2   www.cookbooks.com/Recipe-Details.aspx?id=10570  Gathered  \n",
       "3  www.cookbooks.com/Recipe-Details.aspx?id=897570  Gathered  \n",
       "4  www.cookbooks.com/Recipe-Details.aspx?id=659239  Gathered  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "df_recipes.drop(['NER'],axis=1,inplace=True)\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33aa270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title          1\n",
      "ingredients    0\n",
      "directions     0\n",
      "link           0\n",
      "source         0\n",
      "dtype: int64\n",
      "type        0\n",
      "question    0\n",
      "response    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking missing values\n",
    "print(df_recipes.isnull().sum())\n",
    "print(df_qna.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title          0\n",
      "ingredients    0\n",
      "directions     0\n",
      "link           0\n",
      "source         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Removing the row with a missing value\n",
    "df_recipes = df_recipes.dropna()\n",
    "print(df_recipes.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908a9fe",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d3979",
   "metadata": {},
   "source": [
    "### Converting Datasets to JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Creating data directory (for the standardized datasets - JSON)\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# Cooking knowledge (QnA) preparation\n",
    "def prepare_qna_data():\n",
    "    \n",
    "    # Converting it to structured format (list of dictionaries)\n",
    "    qna_data = []\n",
    "    for _, row in df_qna.iterrows():\n",
    "        qna_item = {\n",
    "            'type': row['type'],\n",
    "            'question': row['question'],\n",
    "            'response': row['response']\n",
    "        }\n",
    "        qna_data.append(qna_item)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open('data/cooking_qna.json', 'w') as f:\n",
    "        json.dump(qna_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(qna_data)} QnA items\")\n",
    "    return qna_data\n",
    "\n",
    "# Recipes dataset preparation\n",
    "def prepare_recipes_data():\n",
    "    \n",
    "    # Cleaning and structuring the data\n",
    "    recipes_data = []\n",
    "    for _, row in df_recipes.iterrows():\n",
    "        # Converting string representations of lists to actual lists\n",
    "        try:\n",
    "            ingredients = json.loads(row['ingredients'].replace(\"'\", \"\\\"\"))\n",
    "            directions = json.loads(row['directions'].replace(\"'\", \"\\\"\"))\n",
    "        except:\n",
    "            # Handle cases where json parsing fails\n",
    "            ingredients = row['ingredients'].strip('[]').split(', ')\n",
    "            directions = row['directions'].strip('[]').split(', ')\n",
    "        \n",
    "        recipe = {\n",
    "            'title': row['title'],\n",
    "            'ingredients': ingredients,\n",
    "            'directions': directions,\n",
    "            'source': row['source'] if 'source' in row else 'Unknown',\n",
    "            'link': row['link'] if 'link' in row else 'Unknown'\n",
    "        }\n",
    "        recipes_data.append(recipe)\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open('data/recipes.json', 'w') as f:\n",
    "        json.dump(recipes_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {len(recipes_data)} recipes\")\n",
    "    return recipes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb71248",
   "metadata": {},
   "source": [
    "### Combining the datasets into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets():\n",
    "\n",
    "    # Loading the prepared datasets - parameter 'r' means 'read only'\n",
    "    with open('data/cooking_qna.json', 'r') as f:\n",
    "        qna_data = json.load(f)\n",
    "    \n",
    "    with open('data/recipes.json', 'r') as f:\n",
    "        recipes_data = json.load(f)\n",
    "    \n",
    "    # Creating text entries for embedding\n",
    "    combined_data = []\n",
    "    \n",
    "    # Process QnA data\n",
    "    for item in qna_data:\n",
    "        combined_data.append({\n",
    "            'id': f\"qna_{len(combined_data)}\",\n",
    "            'content': f\"Question: {item['question']}\\nAnswer: {item['response']}\",\n",
    "            'metadata': {\n",
    "                'type': 'qna',\n",
    "                'category': item['type']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Process recipes data\n",
    "    for recipe in recipes_data:\n",
    "        # Format ingredients as a list\n",
    "        ingredients_text = \"\\n\".join([f\"- {ing}\" for ing in recipe['ingredients']])\n",
    "        \n",
    "        # Format directions as numbered steps\n",
    "        directions_text = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(recipe['directions'])])\n",
    "        \n",
    "        # Combine everything into a single document\n",
    "        recipe_text = f\"Recipe: {recipe['title']}\\n\\nIngredients:\\n{ingredients_text}\\n\\nDirections:\\n{directions_text}\"\n",
    "        \n",
    "        combined_data.append({\n",
    "            'id': f\"recipe_{len(combined_data) - len(qna_data)}\",\n",
    "            'content': recipe_text,\n",
    "            'metadata': {\n",
    "                'type': 'recipe',\n",
    "                'title': recipe['title'],\n",
    "                'ingredients': recipe['ingredients']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Save the combined dataset\n",
    "    with open('data/combined_data.json', 'w') as f:\n",
    "        json.dump(combined_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Created combined dataset with {len(combined_data)} items\")\n",
    "    return combined_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Preparing QnA data...\")\n",
    "    prepare_qna_data()\n",
    "    \n",
    "    print(\"Preparing recipes data...\")\n",
    "    prepare_recipes_data()\n",
    "    \n",
    "    print(\"Combining datasets...\")\n",
    "    combine_datasets()\n",
    "    \n",
    "    print(\"Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3faa565",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b37161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anusk\\anaconda_python\\envs\\data-science\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anusk\\anaconda_python\\envs\\data-science\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36722062",
   "metadata": {},
   "source": [
    "The JSON file contains too many rows - each embedding takes approx. 15-40 ms to generate, meaning that it would take 15-30 hours. It might also crush or run out of memory before finishing. Therefore, I will reduce the data to 50000 rows with an option for further fine-tuning in the future from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e6d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 50000 entries (QnA: 500, Recipes: 49500)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# Opening combined dataset\n",
    "with open('data/combined_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Separating by type – to ensure my sample will contain both QnA and recipes\n",
    "qna_data = [item for item in data if item['metadata']['type'] == 'qna']\n",
    "recipe_data = [item for item in data if item['metadata']['type'] == 'recipe']\n",
    "\n",
    "# Choosing number of samples\n",
    "qna_sample = random.sample(qna_data, 500)\n",
    "recipe_sample = random.sample(recipe_data, 49500)\n",
    "\n",
    "# Combine and shuffle\n",
    "sampled_data = qna_sample + recipe_sample\n",
    "random.shuffle(sampled_data)\n",
    "\n",
    "# Saving to a new file\n",
    "with open('data/sample_combined_data.json', 'w') as f:\n",
    "    json.dump(sampled_data, f, indent=2)\n",
    "\n",
    "print(f\"Sampled {len(sampled_data)} entries (QnA: {len(qna_sample)}, Recipes: {len(recipe_sample)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabea0c7",
   "metadata": {},
   "source": [
    "Now I will generate embeddings for 50.000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4e52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Loading SentenceTransformer (embedding) model...\n",
      "Generating embeddings for 50000 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1563/1563 [44:01<00:00,  1.69s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for 50000 items\n",
      "Embedding dimension: 384\n",
      "Embedding generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Generating embeddings items in my sample\n",
    "\n",
    "def generate_embeddings():\n",
    "    # Loading the combined data\n",
    "    with open('data/sample_combined_data.json', 'r') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    # Creating output directory for embeddings if it doesn't exist\n",
    "    if not os.path.exists('embeddings'):\n",
    "        os.makedirs('embeddings')\n",
    "    \n",
    "    # Sentence transformer model initialization\n",
    "    print(\"Loading SentenceTransformer (embedding) model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # small but effective model\n",
    "    \n",
    "    # Extracting all texts that need embeddings\n",
    "    texts = [item['content'] for item in sample_data]\n",
    "    ids = [item['id'] for item in sample_data]\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} items...\")\n",
    "    \n",
    "    # Generating embeddings in batches to avoid memory issues\n",
    "    all_embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "    \n",
    "    # Creating a dictionary mapping IDs to embeddings\n",
    "    embeddings_dict = {}\n",
    "    for i, item_id in enumerate(ids):\n",
    "        embeddings_dict[item_id] = all_embeddings[i].tolist()\n",
    "    \n",
    "    # Save embeddings\n",
    "    with open('embeddings/embeddings.json', 'w') as f:\n",
    "        json.dump(embeddings_dict, f)\n",
    "    \n",
    "    # Also save as numpy array for FAISS\n",
    "    embeddings_array = np.array(all_embeddings, dtype=np.float32)\n",
    "    np.save('embeddings/embeddings_array.npy', embeddings_array)\n",
    "    \n",
    "    # Save IDs in the same order as embeddings\n",
    "    with open('embeddings/ids.json', 'w') as f:\n",
    "        json.dump(ids, f)\n",
    "    \n",
    "    print(f\"Saved embeddings for {len(ids)} items\")\n",
    "    print(f\"Embedding dimension: {embeddings_array.shape[1]}\")\n",
    "    \n",
    "    return embeddings_array, ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating embeddings...\")\n",
    "    generate_embeddings()\n",
    "    print(\"Embedding generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87641f37",
   "metadata": {},
   "source": [
    "# Vector Database - FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791d1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Using cached faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b7ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7dc74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Built FAISS index with 50000 vectors\n",
      "Vector database setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Making a class that can be called later\n",
    "class VectorDatabase:\n",
    "    def __init__(self, index_path='data/faiss_index'):\n",
    "        self.index_path = index_path\n",
    "        self.index = None\n",
    "        self.ids = []\n",
    "        self.combined_data = []\n",
    "    \n",
    "    # Building a FAISS index from the embeddings\n",
    "    def build_index(self, embeddings_path='embeddings/embeddings_array.npy', \n",
    "                   ids_path='embeddings/ids.json',\n",
    "                   data_path='data/sample_combined_data.json'):\n",
    "    \n",
    "        # Loading data\n",
    "        print(\"Loading embeddings...\")\n",
    "        embeddings = np.load(embeddings_path)\n",
    "\n",
    "        with open(ids_path, 'r') as f:\n",
    "            self.ids = json.load(f)\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.combined_data = json.load(f)\n",
    "        \n",
    "        # Directory for the index if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)\n",
    "        \n",
    "        # Create and add to FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)  # simple L2 distance index\n",
    "        self.index.add(embeddings)  # add vectors to the index\n",
    "        \n",
    "        # Save the index\n",
    "        faiss.write_index(self.index, f\"{self.index_path}.idx\")\n",
    "        \n",
    "        # Save the IDs and data paths for later loading\n",
    "        with open(f\"{self.index_path}_metadata.json\", 'w') as f:\n",
    "            json.dump({\n",
    "                'ids_path': ids_path,\n",
    "                'data_path': data_path\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Built FAISS index with {self.index.ntotal} vectors\")\n",
    "        return self.index\n",
    "    \n",
    "    # Load a previously built FAISS index\n",
    "    def load_index(self):\n",
    "        \n",
    "        # Load saved index and metadata\n",
    "        self.index = faiss.read_index(f\"{self.index_path}.idx\")\n",
    "\n",
    "        with open(f\"{self.index_path}_metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        with open(metadata['ids_path'], 'r') as f:\n",
    "            self.ids = json.load(f)\n",
    "        \n",
    "        # Load sample of combined data\n",
    "        with open(metadata['data_path'], 'r') as f:\n",
    "            self.combined_data = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded FAISS index with {self.index.ntotal} vectors\")\n",
    "        return self.index\n",
    "    \n",
    "    # Function for searching the index for similar vectors\n",
    "    def search(self, query_embedding, k=5):\n",
    "\n",
    "        if self.index is None:\n",
    "            self.load_index()\n",
    "        \n",
    "        # Make sure the query embedding is in the right shape\n",
    "        if len(query_embedding.shape) == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Search the index\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Get the corresponding data\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.ids):  # Ensure index is valid\n",
    "                item_id = self.ids[idx]\n",
    "                \n",
    "                # Find the corresponding item in combined_data\n",
    "                item_data = next((item for item in self.combined_data if item['id'] == item_id), None)\n",
    "                \n",
    "                if item_data:\n",
    "                    results.append({\n",
    "                        'id': item_id,\n",
    "                        'distance': float(distances[0][i]),\n",
    "                        'content': item_data['content'],\n",
    "                        'metadata': item_data['metadata']\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the vector database\n",
    "    vector_db = VectorDatabase()\n",
    "    \n",
    "    # Build the index\n",
    "    vector_db.build_index()\n",
    "    \n",
    "    print(\"Vector database setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4538e4",
   "metadata": {},
   "source": [
    "# Local LMM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cf9665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.9-cp312-cp312-win_amd64.whl\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.9\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7a63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anusk\\anaconda_python\\envs\\data-science\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.3/13.3 MB 6.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.6/13.3 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.5/13.3 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.0/13.3 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.7/13.3 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.4/13.3 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.7/13.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyasn1, proto-plus, httplib2, googleapis-common-protos, cachetools, rsa, pyasn1-modules, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-2.40.1 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6706988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb919d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# for accessing the API key for ChatGPT\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dde9e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   256.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   164.01 MiB\n",
      "llama_context: graph nodes  = 1094\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 50000 vectors\n"
     ]
    }
   ],
   "source": [
    "# Loading local LLM (Mistral 7B Instruct)\n",
    "llm = Llama(model_path=\"models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_ctx=2048, n_threads=8)\n",
    "\n",
    "# Embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector DB\n",
    "db = VectorDatabase()\n",
    "db.load_index()\n",
    "\n",
    "# Set your OpenAI key if not already done\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def ask(question, k=5, max_tokens=256, use_fallback=True):\n",
    "    \n",
    "    # Embed query and search vector DB\n",
    "    query_embedding = embed_model.encode(question)\n",
    "    results = db.search(np.array(query_embedding), k=k)\n",
    "\n",
    "    # Prepare context and prompt\n",
    "    context = \"\\n\\n\".join([r[\"content\"] for r in results])\n",
    "    prompt = f\"\"\"<s>[INST] You are a friendly cooking assistant helping someone who may be a beginner cook. \n",
    "                Always be encouraging, patient, and explain cooking concepts in a simple, easy-to-understand way.\n",
    "                Use these relevant pieces of information (context below) to answer the question:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User's question: {question}\n",
    "\n",
    "    Please provide a concise, friendly, and helpful answer. If the information isn't sufficient to answer the question fully, \n",
    "    just answer with what you know from the provided information. [/INST]</s>\n",
    "    \"\"\"\n",
    "\n",
    "    # Calling local Mistral model\n",
    "    try:\n",
    "        response = llm(prompt, max_tokens=max_tokens, stop=[\"</s>\"])\n",
    "        answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "        # Check if the answer is empty or too vague\n",
    "        if not answer or answer.lower() in [\"I don't know\", \n",
    "            \"I don't have enough information\",\n",
    "            \"I cannot provide\",\n",
    "            \"I'm not sure\",\n",
    "            \"insufficient information\"]:\n",
    "            raise ValueError(\"Low confidence from local model.\")\n",
    "        return {\n",
    "             \"answer\": answer,\n",
    "             \"source\": \"mistral\"\n",
    "        } \n",
    "\n",
    "    except Exception as e:\n",
    "        if use_fallback:\n",
    "            print(\"[!] Local model failed. Falling back to Gemini...\")\n",
    "            gemini_answer = ask_gemini(question, context, max_tokens)\n",
    "            return {\n",
    "                \"answer\": gemini_answer,\n",
    "                \"source\": \"gemini\"\n",
    "            }\n",
    "        else:\n",
    "            raise e\n",
    "        \n",
    "def ask_gemini(question, context, max_tokens = 768):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    full_prompt=f\"\"\"You are a friendly cooking assistant helping someone who may be a beginner cook. \n",
    "                Always be encouraging, patient, and explain cooking concepts in a simple, easy-to-understand way.\n",
    "                Please provide a concise, friendly, and helpful answer.\n",
    "                Use these relevant pieces of information (context below) to answer the question:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User's question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(full_prompt)\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3381237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  115055.51 ms\n",
      "llama_perf_context_print: prompt eval time =  115053.08 ms /  1519 tokens (   75.74 ms per token,    13.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   73753.24 ms /   277 runs   (  266.26 ms per token,     3.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  189186.36 ms /  1796 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: mistral\n",
      "Hi there! I'd be happy to help you learn how to make perfect hard-boiled eggs. Here's a simple recipe that I know you'll love:\n",
      "\n",
      "Ingredients:\n",
      "- Eggs\n",
      "- Water\n",
      "- Ice\n",
      "\n",
      "Directions:\n",
      "1. Put the eggs in a pot large enough so that they will fit in a single layer on the bottom.\n",
      "2. Add enough cold tap water to cover the eggs by two inches.\n",
      "3. Bring the water to a rapid boil, uncovered.\n",
      "4. As soon as the water boils, shut off the heat and cover the pan.\n",
      "5. Let it sit, on the burner, for exactly ten minutes.\n",
      "6. While the eggs are sitting, prepare an ice water bath in a large bowl.\n",
      "7. At ten minutes, dip out the eggs and move them to the ice water bath.\n",
      "8. Let the eggs cool completely.\n",
      "9. Peel and use!\n",
      "\n",
      "Remember, the secret to perfect hard-boiled eggs is to cook them for exactly ten minutes and then cool them in ice water to stop the cooking process. This way, the eggs will be easy to peel and you'll get a perfectly cooked yolk every time. Enjoy your delicious hard-boiled eggs!\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"How to make hard-boiled eggs?\"\n",
    "    result = ask(q, max_tokens=512)\n",
    "    print(f\"Source: {result['source']}\")\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d7ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Local model failed. Falling back to Gemini...\n",
      "Source: gemini\n",
      "Of course!  Let's find some mushroom recipes perfect for beginners.  The key is to pick recipes with clear instructions and not too many complicated steps.\n",
      "\n",
      "Two great options from our list are:\n",
      "\n",
      "* **Champignon Salat Mit Ei (German Mushroom & Egg Salad):** This salad is super easy! You just slice mushrooms, chop some ingredients, whisk a dressing, and assemble.  No cooking of the mushrooms is required, making it perfect for a first-timer.\n",
      "\n",
      "* **Roasted Mushrooms Baked With A Bourbon Cream Sauce:** While it has multiple steps, each one is straightforward. Roasting mushrooms is simple – just toss them with oil and seasonings and bake! The sauce involves some simmering, but the instructions are clear.\n",
      "\n",
      "\n",
      "Don't be afraid to experiment! Mushrooms are incredibly versatile. If you feel confident, the *Croute aux Champignons* is a delicious next step, but the other recipes are great starting points. Remember, cooking is all about having fun and enjoying the process.  Happy cooking!\n"
     ]
    }
   ],
   "source": [
    "# Test 2 - more advanced\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Can you suggest some beginner-friendly champignons recipes?\"\n",
    "    result = ask(q, max_tokens=512)\n",
    "    print(f\"Source: {result['source']}\")\n",
    "    print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
